{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file quantize_model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"yoshitomo-matsubara/bert-base-uncased-sst2\"\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\".\",file_name=\"quantize_model.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9988948702812195}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"I hate you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9995238780975342}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"I like you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length: 128\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "print(f'Payload sequence length: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_clx = pipeline(\"text-classification\",model=model_id)\n",
    "\n",
    "vanilla_model = measure_latency(vanilla_clx)\n",
    "# quantized_model = measure_latency(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 252.65053809989695; Average latency (ms) - 215.75 +\\- 27.86;\n",
      "Quantized model: P95 latency (ms) - 146.056037150629; Average latency (ms) - 139.34 +\\- 17.53;\n",
      "Improvement through quantization: 1.73x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic quantization optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/optimum/onnxruntime/configuration.py:706: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "2023-01-31 16:39:07.855500836 [W:onnxruntime:, inference_session.cc:1458 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n",
      "The ONNX file model_optimized_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTOptimizer, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig, AutoQuantizationConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"yoshitomo-matsubara/bert-base-uncased-sst2\"\n",
    "onnx_path=\"dyn_test\"\n",
    "\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(ort_model)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(onnx_path,file_name=\"model_optimized.onnx\")\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.quantize(\n",
    "    save_dir=onnx_path,\n",
    "    quantization_config=dqconfig,\n",
    ")\n",
    "\n",
    "# load the optimized quantized model\n",
    "opt_model = ORTModelForSequenceClassification.from_pretrained(onnx_path, file_name=\"model_optimized_quantized.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 316.55501350051054; Average latency (ms) - 308.13 +\\- 21.29;\n",
      "Quantized model: P95 latency (ms) - 139.15134559983926; Average latency (ms) - 137.75 +\\- 0.80;\n",
      "Improvement through quantization: 2.27x\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "dyn_q = pipeline(\"sentiment-analysis\", model=opt_model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "quantized_model = measure_latency(dyn_q)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
