{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune open-source ChatGPT alternative\n",
    "\n",
    "In this tutorial, we are going to fine-tune the new [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) on the [ELI5](https://huggingface.co/datasets/eli5) dataset to improve the explanation and question-answering skills of the agent. The [ELI5](https://huggingface.co/datasets/eli5) dataset is an English-language dataset of questions and answers gathered from three subreddits where users ask factual questions requiring paragraph-length or longer answers. We are going to use Hugging Face Transformers and DeepSpeed ZeRO to fine-tune our model.\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "1. Setup Environment\n",
    "2. Create and prepare chat dataset\n",
    "3. fine-tune the GPT model using Deepspeed\n",
    "4. Testing new agent\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "*Note: This tutorial was created and ran on a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB.*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "The first step is installing the Hugging Face Libraries, including transformers, datasets, and DeepSeed. Running the following cell will install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install torch with the correct cuda version, check nvcc --version\n",
    "#!pip install torch --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
    "# install Hugging Face Libraries\n",
    "!pip install \"transformers==4.26.0\" \"datasets==2.9.0\" \"accelerate==0.16.0\" \"evaluate==0.4.0\" --upgrade\n",
    "# install deepspeed and ninja for jit compilations of kernels\n",
    "!pip install \"deepspeed==0.8.0\" ninja --upgrade\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score nltk py7zr tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [ELI5](https://huggingface.co/datasets/eli5) dataset, but before fine-tuning the model, we need to preprocess the data. We will create a \"chat\" version of the dataset by adding `<user>` and `<bot>`tokens and add an end-of-sequence `<|endoftext|>` token to help the model learn to distinguish consecutive examples. Additionally, we create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face. The dataset contains `272634` samples for `eli5`. We will downsample the dataset to `10 000` to make it more realistic for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (/home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-ff13b89bd5550ed9.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "# Load Tokenizer \n",
    "model_id = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset_id = \"eli5\"\n",
    "dataset = load_dataset(dataset_id, split=\"train_eli5\")\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42).select(range(10_000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [ELI5](https://huggingface.co/datasets/eli5) sample can include multiple answers to a ‚Äúquestion‚Äù. We will select the answer with the highest user score for our explanation. \n",
    "\n",
    "*Note: This dataset is a good example of using reinforcement learning for training transformers learning to generate answers with higher scores. Let me know if you are interested in an example of that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-6fdce9b0bad807df.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'If you touch something so hot that it vaporizes your finger would you feel the pain?', 'answer': \"Yes, because something that hot will be heating your arm hot enough to burn you. It doesn't matter if you can get the signal from your finger or not if the rest of your arm and body is on fire.\"}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def filter_score(sample):\n",
    "\t# create new question field\n",
    "\tsample[\"question\"] = sample[\"title\"]\n",
    "\n",
    "\t# get the answer with the highest score\n",
    "\tindex_of_best_score = sample[\"answers\"][\"score\"].index(max(sample[\"answers\"][\"score\"]))\n",
    "\tsample[\"answer\"] = sample[\"answers\"][\"text\"][index_of_best_score]\n",
    "\treturn sample\n",
    "\n",
    "# filter dataset and remove all other columns \n",
    "dataset = dataset.map(filter_score, remove_columns=list(dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(dataset[random.randint(0, 10_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert our dataset into a chat version. Here we will follow the instructions on the [Model card](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B#strengths-of-the-model) and add the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006523847579956055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a8d40d79fd49d68cd3a0e27a47d159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<human>: Explain like I am five: Why does the American school system skip the letter 'E' in their grading system?\\n<bot>: The F stands for fail, and doesn't figure in with the lettering system the same way. Some places do use E<|endoftext|>\"}\n"
     ]
    }
   ],
   "source": [
    "# dataset template for chat conversation\n",
    "template=f'''<human>: Explain like I am five: {{question}}\n",
    "<bot>: {{answer}}{{eos_token}}'''\n",
    "\n",
    "eos_token = tokenizer.eos_token \n",
    "\n",
    "def template_dataset(sample):\n",
    "\tsample[\"text\"] = template.format(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tquestion=sample[\"question\"], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tanswer=sample[\"answer\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\teos_token=eos_token\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\treturn sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(dataset[random.randint(0, 10_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005268573760986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc9cf9b316441ca9b962a2d17194e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004761934280395508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b049fc097a314757a8c8748910a7f24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 902\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to save our processed dataset to disk to load it during fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004867076873779297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 902,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f303e47690b44775823479bf3c89ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset.save_to_disk(\"lm_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012156009674072266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485eded93db345149d55e1890d479b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at togethercomputer/GPT-NeoXT-Chat-Base-20B were not used when initializing GPTNeoXModel: ['embed_out.weight']\n",
      "- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 8 GPUs per node.\n",
      "SW: Model with 20244M total params, 309M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  509.07GB |   1.15GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  905.01GB |   1.15GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  452.50GB |   5.87GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  905.01GB |   5.87GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "   13.85GB |  43.58GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "  905.01GB |  43.58GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
    "model = AutoModel.from_pretrained(\"togethercomputer/GPT-NeoXT-Chat-Base-20B\")\n",
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=8, num_nodes=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the GPT model using Deepspeed\n",
    "\n",
    "We are going to use Hugging Face Transformers and its `Trainer`, which integrates [DeepSpeed ZeRO](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#1-what-is-deepspeed-zero) for efficient training and model parallelism. Therefore we need to create a script `run_clm.py`, which we then launch with `deepspeed` to run distributed training on our 4x A10G GPUs. \n",
    "\n",
    "We prepared a run_clm.py¬†training script based on the¬†[previous blog post](https://www.philschmid.de/fine-tune-flan-t5), which supports our deepspeed config and all other Hyperparameters. For the deepspeed config, we leverage already created configs from the [‚ÄúFine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers‚Äù](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#fine-tune-model-using-deepspeed) example. \n",
    "\n",
    "- [ds_flan_t5_z3_config_bf16.json](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/configs/ds_flan_t5_z3_config_bf16.json)\n",
    "- [ds_flan_t5_z3_offload_bf16.json](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/configs/ds_flan_t5_z3_offload_bf16.json)\n",
    "\n",
    "*Note: If* you are running on NVIDIA V100s, you have to adjust the mixed precision from `bf16` to `fp16`.\n",
    "\n",
    "We have to use the `offload` config for our setup since the model is too big to fit into the GPU memory without offloading to the CPU. \n",
    "\n",
    "We can now launch our training with `deepspeed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2023-03-11 22:25:30,213] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-03-11 22:25:30,250] [INFO] [runner.py:548:main] cmd = /opt/conda/envs/pytorch/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None scripts/run_clm_deepspeed.py --model_id togethercomputer/GPT-NeoXT-Chat-Base-20B --dataset_path lm_dataset --epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --lr 1e-4 --deepspeed configs/ds_flan_t5_z3_offload_bf16.json\n",
      "[2023-03-11 22:25:31,821] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2023-03-11 22:25:31,821] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2023-03-11 22:25:31,821] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2023-03-11 22:25:31,821] [INFO] [launch.py:162:main] dist_world_size=4\n",
      "[2023-03-11 22:25:31,821] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 656/656 [00:00<00:00, 167kB/s]\n",
      "Downloading (‚Ä¶)model.bin.index.json: 100%|‚ñà| 57.7k/57.7k [00:00<00:00, 11.8MB/s]\n",
      "Downloading (‚Ä¶)l-00001-of-00005.bin: 100%|‚ñà‚ñà| 9.95G/9.95G [00:29<00:00, 338MB/s]\n",
      "Downloading (‚Ä¶)l-00002-of-00005.bin: 100%|‚ñà‚ñà| 9.79G/9.79G [00:27<00:00, 350MB/s]\n",
      "Downloading (‚Ä¶)l-00003-of-00005.bin: 100%|‚ñà‚ñà| 9.71G/9.71G [00:32<00:00, 298MB/s]\n",
      "Downloading (‚Ä¶)l-00004-of-00005.bin: 100%|‚ñà‚ñà| 9.71G/9.71G [00:29<00:00, 330MB/s]\n",
      "Downloading (‚Ä¶)l-00005-of-00005.bin: 100%|‚ñà‚ñà| 2.13G/2.13G [00:14<00:00, 149MB/s]\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=8 scripts/run_clm_deepspeed.py \\\n",
    "    --model_id \"togethercomputer/GPT-NeoXT-Chat-Base-20B\" \\\n",
    "    --dataset_path lm_dataset \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --lr 1e-4 \\\n",
    "    --bf16 True \\\n",
    "    --deepspeed configs/ds_flan_t5_z3_offload_bf16.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
