{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate peft evaluate deepspeed loralib rouge-score nltk py7zr --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-a1d6a23de408a6f2.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-e47ec054476ee015.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n",
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np \n",
    "\n",
    "dataset_id = \"samsum\"\n",
    "model_id=\"google/flan-t5-base\"\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id)\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-20 21:52:04,973] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-xl\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/pytorch_model.bin.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[2023-01-20 21:52:17,021] [INFO] [partition_parameters.py:413:__exit__] finished initializing model with 2.85B parameters\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-xl.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "trainable params: 4718592 || all params: 4718592 || trainable%: 100.0\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n",
      "loading file spiece.model from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/spiece.model\n",
      "loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/tokenizer_config.json\n",
      "[2023-01-20 21:52:38,795] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-01-20 21:52:38,899] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-01-20 21:52:38,899] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-01-20 21:52:38,899] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-01-20 21:52:39,026] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-01-20 21:52:39,026] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2023-01-20 21:52:39,026] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "[2023-01-20 21:52:39,137] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-01-20 21:52:39,138] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.49 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:39,138] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.11 GB, percent = 25.2%\n",
      "[2023-01-20 21:52:39,150] [INFO] [stage3.py:114:__init__] Reduce bucket size 500,000,000\n",
      "[2023-01-20 21:52:39,150] [INFO] [stage3.py:115:__init__] Prefetch bucket size 50,000,000\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 1.284179449081421 seconds\n",
      "[2023-01-20 21:52:42,172] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-01-20 21:52:42,173] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:42,173] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.12 GB, percent = 25.3%\n",
      "Parameter Offload: Total persistent parameters: 4970496 in 412 params\n",
      "[2023-01-20 21:52:42,471] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-01-20 21:52:42,471] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.02 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:42,472] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.13 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:42,605] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-01-20 21:52:42,606] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:42,606] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.13 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:42,760] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2023-01-20 21:52:42,761] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:42,761] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:42,888] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-01-20 21:52:42,889] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:42,889] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:43,026] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\n",
      "[2023-01-20 21:52:43,027] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:43,027] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:43,159] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\n",
      "[2023-01-20 21:52:43,159] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:43,160] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:43,322] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\n",
      "[2023-01-20 21:52:43,323] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:52:43,323] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:43,324] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-01-20 21:52:43,582] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-01-20 21:52:43,582] [INFO] [utils.py:832:see_memory_usage] MA 1.86 GB         Max_MA 1.86 GB         CA 2.62 GB         Max_CA 3 GB \n",
      "[2023-01-20 21:52:43,583] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 15.16 GB, percent = 25.3%\n",
      "[2023-01-20 21:52:43,583] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-01-20 21:52:43,583] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-01-20 21:52:43,583] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-01-20 21:52:43,583] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "[2023-01-20 21:52:43,587] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\n",
      "[2023-01-20 21:52:43,587] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-01-20 21:52:43,587] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-01-20 21:52:43,587] [INFO] [config.py:1012:print]   amp_enabled .................. False\n",
      "[2023-01-20 21:52:43,587] [INFO] [config.py:1012:print]   amp_params ................... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6d158aa610>\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   communication_data_type ...... None\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   disable_allgather ............ False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   dump_state ................... False\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-01-20 21:52:43,588] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   fp16_enabled ................. False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   global_rank .................. 0\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   loss_scale ................... 0\n",
      "[2023-01-20 21:52:43,589] [INFO] [config.py:1012:print]   memory_breakdown ............. False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f6d158aa4f0>\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   optimizer_name ............... None\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   optimizer_params ............. None\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   pld_enabled .................. False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   pld_params ................... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   prescale_gradients ........... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   scheduler_name ............... None\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   scheduler_params ............. None\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   sparse_attention ............. None\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   steps_per_print .............. inf\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   train_batch_size ............. 1\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   world_size ................... 1\n",
      "[2023-01-20 21:52:43,590] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True\n",
      "[2023-01-20 21:52:43,591] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
      "[2023-01-20 21:52:43,591] [INFO] [config.py:1012:print]   zero_enabled ................. True\n",
      "[2023-01-20 21:52:43,591] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\n",
      "[2023-01-20 21:52:43,591] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004792213439941406 seconds\n",
      "  0%|                                                 | 0/73660 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "  0%|                                     | 13/73660 [01:12<97:55:52,  4.79s/it]^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3214 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/accelerate_lora_t5.py\", line 183, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/accelerate_lora_t5.py\", line 180, in main\n",
      "    training_function(args)  \n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/accelerate_lora_t5.py\", line 127, in training_function\n",
      "    outputs = model(**batch)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1836, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/peft/peft_model.py\", line 503, in forward\n",
      "    return self.base_model(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/peft/tuners/lora.py\", line 142, in forward\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1611, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 1040, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 725, in forward\n",
      "    hidden_states = self.layer[-1](hidden_states)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1212, in _call_impl\n",
      "    result = forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\", line 329, in forward\n",
      "    hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1201, in _call_impl\n",
      "    result = hook(self, input)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 348, in _pre_forward_module_hook\n",
      "    self.pre_sub_module_forward_function(module)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 478, in pre_sub_module_forward_function\n",
      "    param_coordinator.fetch_sub_module(sub_module)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 349, in fetch_sub_module\n",
      "    self.__all_gather_params(params_to_prefetch)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 399, in __all_gather_params\n",
      "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 846, in all_gather_coalesced\n",
      "    param.ds_tensor.to(torch.cuda.current_device()),\n",
      "KeyboardInterrupt\n",
      "  0%|                                    | 13/73660 [01:14<117:11:12,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file configs/accelerate_ds_z3.yaml \\\n",
    "    scripts/accelerate_lora_t5.py \\\n",
    "    --model_id google/flan-t5-xl \\\n",
    "    --dataset_path data \\\n",
    "    --epochs 3 \\\n",
    "    --train_batch_size 2 \\\n",
    "    --eval_batch_size 2 \\\n",
    "    --lr 3e-3     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
