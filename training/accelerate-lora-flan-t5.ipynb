{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.9/site-packages (0.15.0)\n",
      "Collecting peft\n",
      "  Downloading peft-0.0.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: evaluate in /opt/conda/envs/pytorch/lib/python3.9/site-packages (0.4.0)\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.8.0.tar.gz (749 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.9/749.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting loralib\n",
      "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting py7zr\n",
      "  Downloading py7zr-0.20.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (4.63.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Collecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: py-cpuinfo in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from deepspeed) (1.10.2)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tensorboard) (65.6.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tensorboard) (3.20.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tensorboard) (2.15.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tensorboard) (2.2.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from tensorboard) (0.38.4)\n",
      "Collecting texttable\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.0/379.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1\n",
      "  Downloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting inflate64>=0.3.1\n",
      "  Downloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (4.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Building wheels for collected packages: deepspeed, rouge-score\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752141 sha256=907bbdd18548908ea562fd4210c4728db1a3b10a98eb2570276c6823290f0052\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/13/19/1e/77f993df19bfecab5f773450661680e7d3ce3a5e5a44d53a30\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=14ffa6eb3d0e418c5eb70eb57c08547a9f1055b22d2faa6e475909db53118d50\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b0/3f/ac/cc3bc304f50c77ef38d79d8e4e2684313de39af543cb4eb3da\n",
      "Successfully built deepspeed rouge-score\n",
      "Installing collected packages: texttable, tensorboard-plugin-wit, ninja, hjson, brotli, tensorboard-data-server, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, loralib, inflate64, grpcio, absl-py, rouge-score, py7zr, markdown, deepspeed, google-auth-oauthlib, tensorboard, peft\n",
      "Successfully installed absl-py-1.4.0 brotli-1.0.9 deepspeed-0.8.0 google-auth-oauthlib-0.4.6 grpcio-1.51.1 hjson-3.1.0 inflate64-0.3.1 loralib-0.1.1 markdown-3.4.1 multivolumefile-0.2.3 ninja-1.11.1 nltk-3.8.1 peft-0.0.2 py7zr-0.20.2 pybcj-1.0.1 pycryptodomex-3.16.0 pyppmd-1.0.0 pyzstd-0.15.3 rouge-score-0.1.2 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 texttable-1.6.7\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft evaluate deepspeed loralib rouge-score nltk py7zr --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset_id = \"samsum\"\n",
    "model_id=\"google/flan-t5-base\"\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id)\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-20 21:49:27,275] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-xl\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/pytorch_model.bin.index.json\n",
      "Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[2023-01-20 21:49:38,201] [INFO] [partition_parameters.py:413:__exit__] finished initializing model with 2.85B parameters\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-xl.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "trainable params: 4718592 || all params: 4718592 || trainable%: 100.0\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n",
      "loading file spiece.model from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/spiece.model\n",
      "loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/efd6532890a60712aadbb897ba688258beacbc78/tokenizer_config.json\n",
      "[2023-01-20 21:49:59,428] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-01-20 21:49:59,535] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-01-20 21:49:59,536] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-01-20 21:49:59,536] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-01-20 21:49:59,670] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-01-20 21:49:59,671] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2023-01-20 21:49:59,671] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
      "[2023-01-20 21:49:59,795] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-01-20 21:49:59,796] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.49 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:49:59,796] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.93 GB, percent = 30.0%\n",
      "[2023-01-20 21:49:59,807] [INFO] [stage3.py:114:__init__] Reduce bucket size 500,000,000\n",
      "[2023-01-20 21:49:59,807] [INFO] [stage3.py:115:__init__] Prefetch bucket size 50,000,000\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 1.526719093322754 seconds\n",
      "[2023-01-20 21:50:03,292] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-01-20 21:50:03,293] [INFO] [utils.py:832:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:03,293] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.94 GB, percent = 30.0%\n",
      "Parameter Offload: Total persistent parameters: 4970496 in 412 params\n",
      "[2023-01-20 21:50:03,581] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-01-20 21:50:03,582] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.02 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:03,582] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.94 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:03,716] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-01-20 21:50:03,717] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:03,717] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.94 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:03,877] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2023-01-20 21:50:03,878] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:03,878] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,012] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-01-20 21:50:04,013] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:04,013] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,154] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\n",
      "[2023-01-20 21:50:04,155] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:04,155] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,290] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\n",
      "[2023-01-20 21:50:04,291] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:04,291] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,457] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\n",
      "[2023-01-20 21:50:04,458] [INFO] [utils.py:832:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.76 GB         Max_CA 1 GB \n",
      "[2023-01-20 21:50:04,458] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,458] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-01-20 21:50:04,720] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-01-20 21:50:04,721] [INFO] [utils.py:832:see_memory_usage] MA 1.86 GB         Max_MA 1.86 GB         CA 2.62 GB         Max_CA 3 GB \n",
      "[2023-01-20 21:50:04,721] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 17.98 GB, percent = 30.0%\n",
      "[2023-01-20 21:50:04,722] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-01-20 21:50:04,722] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-01-20 21:50:04,722] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2023-01-20 21:50:04,722] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[(0.9, 0.999)]\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1012:print]   amp_enabled .................. False\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1012:print]   amp_params ................... False\n",
      "[2023-01-20 21:50:04,726] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f65b5069670>\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   communication_data_type ...... None\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   disable_allgather ............ False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   dump_state ................... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-01-20 21:50:04,727] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   fp16_enabled ................. False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   global_rank .................. 0\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   loss_scale ................... 0\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   memory_breakdown ............. False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f65b5069550>\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-01-20 21:50:04,728] [INFO] [config.py:1012:print]   optimizer_name ............... None\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   optimizer_params ............. None\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   pld_enabled .................. False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   pld_params ................... False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   prescale_gradients ........... False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   scheduler_name ............... None\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   scheduler_params ............. None\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   sparse_attention ............. None\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   steps_per_print .............. inf\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   train_batch_size ............. 1\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   world_size ................... 1\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  True\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   zero_enabled ................. True\n",
      "[2023-01-20 21:50:04,729] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\n",
      "[2023-01-20 21:50:04,730] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004792213439941406 seconds\n",
      "  0%|                                                 | 0/73660 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "  0%|                                     | 1/73660 [00:06<133:38:04,  6.53s/it]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file configs/accelerate_ds_z3.yaml \\\n",
    "    scripts/accelerate_lora_t5.py \\\n",
    "    --model_id google/flan-t5-xl \\\n",
    "    --dataset_path data \\\n",
    "    --epochs 5 \\\n",
    "    --train_batch_size 1 \\\n",
    "    --eval_batch_size 1 \\\n",
    "    --lr 3e-3     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
